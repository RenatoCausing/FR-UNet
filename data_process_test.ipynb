{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71da5f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 10 previews to D:\\FR-UNet\\FR-UNet\\sampled_previews\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from data_process import (\n",
    "    _apply_fov_mask,\n",
    "    _build_lookup,\n",
    "    _extract_sample_id,\n",
    "    _is_image_file,\n",
    "    _preprocess_retinal_image,\n",
    "    normalization,\n",
    ")\n",
    "\n",
    "\n",
    "# Edit this list to adjust the default CLAHE clip limits and kernel sizes used by the sampler.\n",
    "DEFAULT_CLAHE_CONFIGS: list[tuple[float, int]] = [\n",
    "    (2.0, 8),\n",
    "    (4.0, 16),\n",
    "]\n",
    "\n",
    "\n",
    "def _collect_originals(original_dir: Path) -> dict[str, Path]:\n",
    "    originals: dict[str, Path] = {}\n",
    "    for file_name in sorted(original_dir.iterdir()):\n",
    "        if not _is_image_file(file_name.name):\n",
    "            continue\n",
    "        sample_id = _extract_sample_id(file_name.name)\n",
    "        if not sample_id.isdigit():\n",
    "            continue\n",
    "        originals[sample_id] = file_name\n",
    "    return originals\n",
    "\n",
    "\n",
    "def _select_samples(originals: dict[str, Path], segmented_lookup: dict[str, str], sample_size: int,\n",
    "                    seed: int | None) -> list[str]:\n",
    "    shared_ids = sorted(set(originals.keys()) & set(segmented_lookup.keys()))\n",
    "    if not shared_ids:\n",
    "        raise RuntimeError(\"No overlapping Original/Segmented pairs were found.\")\n",
    "    rng = random.Random(seed)\n",
    "    if sample_size >= len(shared_ids):\n",
    "        return shared_ids\n",
    "    return rng.sample(shared_ids, sample_size)\n",
    "\n",
    "\n",
    "def _weighted_grayscale(image: np.ndarray) -> np.ndarray:\n",
    "    if image.ndim == 3 and image.shape[2] >= 3:\n",
    "        return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return image.copy()\n",
    "\n",
    "\n",
    "def _apply_custom_clahe(image: np.ndarray, clip_limit: float, tile_size: int) -> np.ndarray:\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=(tile_size, tile_size))\n",
    "    enhanced = clahe.apply(image)\n",
    "    return enhanced\n",
    "\n",
    "\n",
    "def _prepare_sample(sample_id: str, paths: dict[str, Path], segmented_lookup: dict[str, str],\n",
    "                    mask_lookup: dict[str, str], tensor_transform: ToTensor,\n",
    "                    clahe_configs: list[tuple[float, int]]) -> dict:\n",
    "    img_path = paths[sample_id]\n",
    "    seg_path = Path(segmented_lookup[sample_id])\n",
    "    mask_path = Path(mask_lookup[sample_id]) if sample_id in mask_lookup else None\n",
    "\n",
    "    raw = cv2.imread(str(img_path), cv2.IMREAD_UNCHANGED)\n",
    "    if raw is None:\n",
    "        raise RuntimeError(f\"Failed to read {img_path}\")\n",
    "    if raw.ndim == 3:\n",
    "        grayscale = _weighted_grayscale(raw)\n",
    "    else:\n",
    "        grayscale = raw.copy()\n",
    "\n",
    "    processed_variants = [_apply_custom_clahe(grayscale, clip, tile) for clip, tile in clahe_configs]\n",
    "    mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE) if mask_path else None\n",
    "    masked_variants = [_apply_fov_mask(proc, mask) for proc in processed_variants]\n",
    "\n",
    "    gt = cv2.imread(str(seg_path), cv2.IMREAD_GRAYSCALE)\n",
    "    if gt is None:\n",
    "        raise RuntimeError(f\"Failed to read segmented map {seg_path}\")\n",
    "\n",
    "    return {\n",
    "        \"sample_id\": sample_id,\n",
    "        \"original_gray\": grayscale,\n",
    "        \"processed_variants\": masked_variants,\n",
    "        \"tensors\": [tensor_transform(masked) for masked in masked_variants],\n",
    "        \"segmented\": gt,\n",
    "        \"mask\": mask,\n",
    "        \"paths\": {\n",
    "            \"original\": str(img_path),\n",
    "            \"segmented\": str(seg_path),\n",
    "            \"mask\": str(mask_path) if mask_path else None,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "def _stack_columns(columns: list[torch.Tensor | None]) -> torch.Tensor:\n",
    "    tensors = [col for col in columns if col is not None]\n",
    "    if not tensors:\n",
    "        raise RuntimeError(\"No tensors provided for concatenation\")\n",
    "    return torch.cat(tensors, dim=-1)\n",
    "\n",
    "\n",
    "def _save_outputs(samples: list[dict], normalized_groups: list[list[torch.Tensor]], output_dir: Path) -> None:\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    metadata: list[dict] = []\n",
    "    for idx, (sample, norm_group) in enumerate(zip(samples, normalized_groups), start=1):\n",
    "        sample_dir = output_dir / f\"{idx:02d}_{sample['sample_id']}\"\n",
    "        sample_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        original_tensor = torch.from_numpy(sample[\"original_gray\"]).unsqueeze(0).float() / 255.0\n",
    "        segmented_tensor = torch.from_numpy(sample[\"segmented\"]).unsqueeze(0).float() / 255.0\n",
    "\n",
    "        concatenated = _stack_columns([\n",
    "            original_tensor,\n",
    "            norm_group[0],\n",
    "            norm_group[1] if len(norm_group) > 1 else None,\n",
    "            segmented_tensor,\n",
    "        ])\n",
    "        concat_img = (concatenated.squeeze().numpy() * 255.0).clip(0, 255).astype(\"uint8\")\n",
    "        cv2.imwrite(str(sample_dir / \"preview.png\"), concat_img)\n",
    "\n",
    "        metadata.append({\n",
    "            \"sample_id\": sample[\"sample_id\"],\n",
    "            \"paths\": sample[\"paths\"],\n",
    "            \"output_dir\": str(sample_dir),\n",
    "            \"has_mask\": sample[\"mask\"] is not None,\n",
    "        })\n",
    "\n",
    "    with (output_dir / \"metadata.json\").open(\"w\", encoding=\"utf-8\") as fp:\n",
    "        json.dump(metadata, fp, indent=2)\n",
    "\n",
    "\n",
    "def _parse_args(argv: list[str] | None = None) -> argparse.Namespace:\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=(\n",
    "            \"Sample a handful of ALL dataset images, apply CLAHE + masking + normalization, and \"\n",
    "            \"write intermediate products for visual inspection.\"\n",
    "        )\n",
    "    )\n",
    "    default_clahe_str = \" \".join(f\"{clip} {tile}\" for clip, tile in DEFAULT_CLAHE_CONFIGS)\n",
    "    parser.add_argument(\n",
    "        \"-dp\",\n",
    "        \"--dataset-path\",\n",
    "        default=r\"D:\\\\DRIVE\\\\2nd RUN\\\\ALL_2\",\n",
    "        help=\"Root folder containing Original/Mask/Segmented subfolders.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-o\",\n",
    "        \"--output-dir\",\n",
    "        default=\"sampled_previews\",\n",
    "        help=\"Directory where preview images will be written (default: sampled_previews).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-n\",\n",
    "        \"--sample-size\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        help=\"Number of random samples to export (default: 10).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-s\",\n",
    "        \"--seed\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"Random seed for reproducibility (default: None).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--clahe\",\n",
    "        nargs=4,\n",
    "        type=float,\n",
    "        metavar=(\"clip1\", \"tile1\", \"clip2\", \"tile2\"),\n",
    "        default=None,\n",
    "        help=(\n",
    "            \"Override CLAHE settings as clip/tile pairs (\"\n",
    "            f\"default: {default_clahe_str}).\"\n",
    "        ),\n",
    "    )\n",
    "    args, _ = parser.parse_known_args(argv)\n",
    "    return args\n",
    "\n",
    "\n",
    "def main(argv: list[str] | None = None) -> None:\n",
    "    args = _parse_args(argv)\n",
    "\n",
    "    base_path = Path(args.dataset_path)\n",
    "    original_dir = base_path / \"Original\"\n",
    "    segmented_dir = base_path / \"Segmented\"\n",
    "    mask_dir = base_path / \"Mask\"\n",
    "\n",
    "    if not original_dir.exists() or not segmented_dir.exists():\n",
    "        raise FileNotFoundError(\"Original and Segmented folders must exist under the dataset path.\")\n",
    "\n",
    "    originals = _collect_originals(original_dir)\n",
    "    segmented_lookup = _build_lookup(str(segmented_dir), \"_segmented\")\n",
    "    mask_lookup = _build_lookup(str(mask_dir), \"_mask\") if mask_dir.exists() else {}\n",
    "\n",
    "    if args.clahe is not None:\n",
    "        clip1, tile1, clip2, tile2 = args.clahe\n",
    "        clahe_configs = [(clip1, int(tile1)), (clip2, int(tile2))]\n",
    "    else:\n",
    "        clahe_configs = list(DEFAULT_CLAHE_CONFIGS)\n",
    "\n",
    "    selected_ids = _select_samples(originals, segmented_lookup, args.sample_size, args.seed)\n",
    "    to_tensor = ToTensor()\n",
    "    samples: list[dict] = []\n",
    "    for sample_id in selected_ids:\n",
    "        samples.append(_prepare_sample(sample_id, originals, segmented_lookup, mask_lookup, to_tensor, clahe_configs))\n",
    "\n",
    "    normalized_groups: list[list[torch.Tensor]] = []\n",
    "    for sample in samples:\n",
    "        normalized = normalization(sample[\"tensors\"])\n",
    "        normalized_groups.append(normalized)\n",
    "    _save_outputs(samples, normalized_groups, Path(args.output_dir))\n",
    "\n",
    "    print(f\"Wrote {len(samples)} previews to {Path(args.output_dir).resolve()}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
